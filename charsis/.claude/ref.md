# Charsis 项目文档

## 项目概述

**项目名称**: Charsis - 刻本图片文字处理与分析
**主要目标**: 对古籍刻本图片进行一系列处理与分析，包括图像预处理、文字切割、OCR识别以及数据分析
**开发语言**: Python
**当前版本**: v4.0 (完整流水线 + 批量推理版本)

## 项目结构

```
charsis/
├── pipeline                 # 统一流水线工具 (可执行)
├── data/                    # 数据目录
│   ├── raw/                # 原始输入图片
│   └── results/            # 统一结果输出目录
│       ├── preprocessed/   # 预处理结果
│       ├── preocr/         # 区域检测结果
│       ├── segments/       # 字符切割结果
│       └── postocr/        # 质量过滤结果
├── src/                    # 核心源代码
│   ├── preprocess/         # 图像预处理模块
│   ├── preocr/             # 区域检测模块 (远程服务)
│   ├── segmentation/       # 字符分割模块 (核心功能)
│   │   ├── vertical_hybrid.py  # 混合分割算法
│   │   ├── projection.py   # 投影分析分割
│   │   └── ...             # 其他分割子模块
│   ├── postocr/            # 质量过滤模块 (v4.0新增)
│   │   ├── core.py         # PostOCR主逻辑
│   │   ├── batch_client.py # 批量推理客户端
│   │   └── vision_client.py # 实时推理客户端
│   ├── pipeline.py         # 流水线主程序
│   ├── config.py           # 统一配置文件
│   └── utils/              # 通用工具
├── notebooks/              # Jupyter分析笔记本
└── tests/                  # 单元测试
```

## 核心功能模块

### 完整处理流水线

**流程**: preprocess → preocr → segment → postocr

1. **预处理 (preprocess)** - 图像去噪、矫正
2. **区域检测 (preocr)** - 远程服务检测文本区域
3. **字符分割 (segment)** - 精细字符切割
4. **质量过滤 (postocr)** - VLM质量评估和筛选

**使用方式**:
```bash
./pipeline all input.jpg                          # 完整流程
./pipeline segment data/results/preocr/dataset/   # 单独运行某阶段
./pipeline postocr                                # 质量过滤
./pipeline postocr status <batch_id>              # 批量任务管理
```

### 1. 图像预处理模块 (preprocess/)
**功能**: 图像增强和标准化
- 灰度转换
- CLAHE局部对比度增强 (clipLimit=2.0, tileGridSize=(8,8))
- 线性变换调整全局对比度和亮度 (alpha=1.5, beta=10)

### 2. 字符分割模块 (segmentation/) - 核心模块

**分割流程**:
1. 图像二值化和形态学操作
2. 轮廓检测和边界框提取
3. 字符尺寸统计分析
4. 纵向连接字符检测与水平投影分割
5. 过宽字符检测与垂直投影分割
6. 窄字符合并处理
7. 结果标注和可视化

**字符分类系统**:
- 🔴 **红色框**: 较窄字符（宽度 < 平均宽度 - 标准差）
- 🟢 **绿色框**: 正常字符
- 🔵 **蓝色框**: 过宽字符（需要垂直分割）
- 🟣 **紫色框**: 纵向连接字符（需要水平分割）
- 🟠 **橙色框**: 红红合并字符
- 🟡 **黄色框**: 红绿合并字符

### 3. 质量过滤模块 (postocr/) - v4.0新增

**功能**: 使用VLM对切割字符进行质量评估和筛选

**核心特性**:
- **双模式支持**: Realtime (实时) / Batch (批量)
- **VLM提供商**: 支持 Doubao、Qwen (qwen3-vl-plus)
- **批量推理**: 成本降低50%，无RPM限制
- **断点恢复**: 支持任务中断和恢复
- **结构化输出**: JSON格式的评估结果

**评估标准**:
```json
{
  "single_character": true,
  "recognizable": true,
  "has_noise": false,
  "character": "字"
}
```

**批量推理工作流**:
1. 生成JSONL (base64编码图片)
2. 上传到API服务
3. 创建批量任务
4. 轮询任务状态 (60秒间隔)
5. 下载结果并解析
6. 过滤和重命名字符图片

**目录结构保持**:
- 输入: `segments/dataset/subdir/region_001/char_001.png`
- 输出: `postocr/dataset/subdir/region_001/001_字.png`
- 支持多层目录结构自动保持 (v4.0.1修复)

## 技术架构

### v2.0投影分析核心改进

**改进前问题**:
- 过度依赖统计阈值，对图像质量敏感
- 投影分析平滑参数固定，分割点选择过于简单
- 缺乏上下文信息和分割验证

**核心技术改进**:

1. **高斯平滑**: 替代简单均值滤波，更好保持边界信息
2. **自适应阈值**: 基于四分位数的动态计算，适应不同密度文本
3. **智能分割点选择**: 多候选点评分机制，综合投影值、位置合理性和局部最小值
4. **谷值检测算法**: 使用scipy.signal.find_peaks检测投影曲线谷值
5. **分割结果验证**: 最小段长度检查和分割数量限制

**性能提升**:
- 阈值计算准确性提升213%
- 过度分割减少20-30%
- 字符尺寸标准差降低38%

### 配置系统

**文件**: `src/config.py`  
**特点**: 集中化参数管理，支持37个精细化配置参数

**主要配置区块**:
- `PROJECTION_CONFIG` - 投影分析参数 (v2.0新增9个改进参数)
- `VERTICAL_CHAR_CONFIG` - 纵向连接字符检测参数
- `WIDE_CHAR_CONFIG` - 过宽字符检测参数
- `RED_RED_MERGE_CONFIG` / `RED_GREEN_MERGE_CONFIG` - 字符合并参数
- `CHAR_CLASSIFICATION_CONFIG` - 字符分类配置

## 开发历程与经验总结

### ✅ 已完成的开发阶段

#### 第一阶段：基础功能实现
- 项目结构搭建
- 图像预处理模块
- 字符分割核心算法
- 字符合并策略

#### 第二阶段：参数配置化
- 将硬编码参数移至配置文件
- 统一参数管理
- 提高代码可维护性

#### 第三阶段：代码重构优化
- segmentation模块化重构
- 代码结构优化
- 功能模块分离

#### 第四阶段：投影分析算法优化
- 改进投影分析的平滑算法
- 实现自适应阈值计算
- 优化分割点选择机制
- 增强分割结果验证

#### 第五阶段：完整流水线开发 (v4.0)
- 集成PreOCR区域检测远程服务
- 开发PostOCR质量过滤模块
- 实现批量推理和断点恢复
- 创建统一流水线CLI工具
- 支持多层目录结构处理

### 🔄 重要调整与学习

#### 列感知方案的探索与回归

**问题发现**: 在尝试实施基于列结构的分层分割策略时，发现该方案存在过度合并问题：
- 字符数从405减少到269，33%的过度合并率
- 误解了以减少字符数量为优化目标，而非提升分割准确性
- 在已经表现良好的系统上强加复杂功能

**重要经验总结**:
1. **以实际效果为准**: 视觉质量和实用性比理论优化更重要
2. **如果没坏，就别修**: 在没有明确问题的情况下，不要盲目添加复杂功能  
3. **准确性>数量减少**: 保持分割的正确性比减少字符数量更重要
4. **渐进式改进**: 先验证问题存在，再设计解决方案

**解决方案**: 使用git恢复到v2.0传统方案，确认其效果优秀

### 📈 纵向连接字符检测优化

**问题发现**: 通过仔细观察标注图像，发现存在明显的纵向连接字符未被正确分割

**参数优化历程**:
```
检测参数调整:
height_multiplier: 1.5 → 1.0 (降低高度阈值)  
aspect_ratio_threshold: 2.5 → 1.8 (降低高宽比要求)
min_width_ratio: 0.3 → 0.1 (大幅降低宽度限制)

效果改进:
纵向连接字符检测: 10个 → 28个+ 
最终字符数: 405 → 437 (增加32个字符)
```

**已完成的优化**:
- ✅ 检测算法敏感度大幅提升
- ✅ 捕获更多细窄的纵向连接字符
- ✅ 字符数合理增长，无过度分割

### 📁 多层目录结构处理优化 (v4.0.1)

**问题发现**: 在处理多级目录结构时，segment阶段只保留dataset名称，丢失中间子目录信息

**问题示例**:
```
输入: data/results/preocr/01_《尚书正义》/01a_preprocessed/region_images/region_001.jpg
错误输出: segments/01_《尚书正义》/region_001/
正确输出: segments/01_《尚书正义》/01a_preprocessed/region_001/
```

**修复方案**:
- 提取 `preocr/<dataset>/.../<subdirs>/region_images/` 之间的完整路径
- 使用 `os.path.join(*parts[idx+1:region_idx])` 保持目录层次
- 验证5个子目录(01a~01e_preprocessed)均正确保持结构

**影响范围**:
- ✅ preprocess阶段: 已使用`os.path.relpath()`，无问题
- ✅ preocr阶段: 已使用`os.path.relpath()`，无问题
- ✅ segment阶段: 已修复 (v4.0.1)
- ✅ postocr阶段: 已使用`os.path.relpath()`，无问题

## OCR集成开发阶段 (v3.0)

### 🔄 Remote PaddleOCR集成已完成

**集成架构**:
- **本地端**: Mac客户端，通过REST API调用远程服务
- **服务端**: Windows + WSL2 + RTX 3090，运行PaddleOCR PP-OCRv5服务
- **网络**: 通过蒲公英内网穿透连接 (http://172.16.1.154:8000)

**核心文件**:
- `src/ocr/remote_paddleocr.py` - 远程PaddleOCR客户端
- `visualize_paddleocr_results.py` - PaddleOCR结果可视化
- `test_demo_recognition.py` - PaddleOCR完整文档识别测试

**性能对比分析**:

| 项目 | Segmentation方案 | PaddleOCR方案 |
|------|-----------------|---------------|
| **处理单元** | 字符级分割 | 文档级识别 |
| **识别数量** | 437个字符 | 22个文本区域 |
| **平均置信度** | N/A | 0.898 |
| **高置信度率** | N/A | 90.9% (≥0.8) |
| **总字符数** | 437个 | 427个 |
| **处理时间** | ~数秒 | 0.67秒 |
| **GPU要求** | 无 | RTX 3090 |

### 📊 当前系统性能 (双轨方案)

**Segmentation方案** (基于demo图像):
- 初始检测字符数: 467个
- 最终字符数: 437个  
- 合并次数: 70次
- 处理质量: 分割精确，边界清晰

**PaddleOCR方案** (基于demo图像):
- 识别区域: 22个
- 总字符数: 427个
- 平均置信度: 0.898
- 处理质量: 高精度文档级识别

### 🎯 集成方案规划

#### 阶段1: 双轨并行运行 ✅
- [x] 保持原有Segmentation功能
- [x] 集成Remote PaddleOCR功能
- [x] 可视化对比分析工具

#### 阶段2: 混合处理流程
- [ ] 设计Segmentation + PaddleOCR混合流程
- [ ] 字符级与区域级结果映射
- [ ] 质量评估和结果选择机制

#### 阶段3: 智能切换策略
- [ ] 基于图像特征自动选择最优方案
- [ ] 置信度阈值动态调整
- [ ] 错误检测和回退机制

## 使用说明

### 环境依赖
```bash
# 核心依赖
pip install opencv-python-headless matplotlib numpy scipy
```

### 基本使用
```bash
# 激活虚拟环境
source .venv/bin/activate

# 图像预处理
python src/preprocess/core.py

# 字符分割 (核心功能)
python src/segmentation/core.py
```

### 配置调整
通过修改 `src/config.py` 中的参数来调整算法行为，支持37个精细化配置参数。

## 项目特点

1. **模块化设计** - 各功能模块独立，接口清晰
2. **配置驱动** - 参数集中管理，易于调优
3. **渐进式改进** - 保持向后兼容，支持原版回退
4. **结果可视化** - 提供详细的标注和统计信息
5. **专门优化** - 针对古籍刻本图像特点进行专门优化

---

*最后更新: 当前版本为v4.0.1，完整流水线已实现，批量推理功能已集成，多层目录结构处理已修复。*